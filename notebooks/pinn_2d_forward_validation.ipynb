{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 2D Elastic Wave PINN Forward Problem Validation\n",
    "\n",
    "## 概要\n",
    "\n",
    "本ノートブックは、Phase 2（pinn-2d-fdtd-integration）で実装された2D弾性波PINNコンポーネントを用いて、順問題の完全なワークフローを実証します。\n",
    "\n",
    "### 目的\n",
    "\n",
    "- FDTDデータを用いた2D PINN訓練の実行\n",
    "- 訓練中の損失項（L_data, L_pde, L_bc）モニタリング\n",
    "- R²スコアによる定量評価\n",
    "- 2D波動場の空間分布可視化\n",
    "- 誤差分布の空間解析\n",
    "\n",
    "### 前提条件\n",
    "\n",
    "- Phase 2実装（pinn-2d-fdtd-integration）が完了していること\n",
    "- `/PINN_data/`に.npzファイル（最低2ファイル）が配置されていること\n",
    "- GPU環境（CUDA 12.4対応）推奨\n",
    "\n",
    "### 実行環境\n",
    "\n",
    "- Python 3.11+\n",
    "- PyTorch 2.4.0 with CUDA 12.4\n",
    "- DeepXDE 1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 0: セットアップとインポート\n",
    "\n",
    "Phase 2実装のServiceクラスをimportし、再現性を確保するためにrandom seedを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# PINN module imports (Phase 2 implementation)\n",
    "from pinn.data.fdtd_loader import FDTDDataLoaderService\n",
    "from pinn.data.dimensionless_scaler import CharacteristicScales, DimensionlessScalerService\n",
    "from pinn.models.pinn_model_builder_2d import PINNModelBuilder2DService\n",
    "from pinn.models.pde_definition_2d import PDEDefinition2DService\n",
    "from pinn.training.train_2d import TrainingPipelineService\n",
    "from pinn.training.callbacks import LossLoggingCallback, R2ValidationCallback, DivergenceDetectionCallback\n",
    "from pinn.validation.r2_score import R2ScoreCalculator\n",
    "from pinn.validation.plot_generator import PlotGeneratorService\n",
    "from pinn.utils.seed_manager import SeedManager\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 10)\n",
    "\n",
    "# Set random seed for reproducibility (Requirement 7.4)\n",
    "SEED = 42\n",
    "SeedManager.set_seed(SEED)\n",
    "print(f\"Random seed set to: {SEED}\")\n",
    "\n",
    "# Check GPU availability (Requirement 2.9)\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"WARNING: GPU not available, training will be slow on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 1: FDTDデータの読み込みと無次元化\n",
    "\n",
    "このステップでは、`/PINN_data/`ディレクトリからFDTDシミュレーションデータを読み込み、無次元化を適用します。\n",
    "\n",
    "### タスク2.1: データファイル読み込み\n",
    "- FDTDデータファイルの検索と読み込み\n",
    "- データディレクトリの存在確認（Requirement 1.7）\n",
    "- データセットサイズの表示（Requirement 1.2）\n",
    "\n",
    "### タスク2.2: 無次元化と変数範囲表示\n",
    "- 特性スケールの計算（Requirement 1.3）\n",
    "- 無次元化後の変数範囲表示（Requirement 1.4）\n",
    "\n",
    "### タスク2.3: Train/Val分割\n",
    "- 訓練データとバリデーションデータの分割（Requirement 1.5）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1: FDTD data file loading (Requirements 1.1, 1.2, 1.7)\n",
    "# Data directory\n",
    "data_dir = Path(\"/PINN_data\")\n",
    "\n",
    "# Check if data directory exists (Requirement 1.7)\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data directory not found: {data_dir}\\n\"\n",
    "        \"Please create /PINN_data/ and place FDTD .npz files there.\\n\"\n",
    "        \"Expected naming: p{pitch_um}_d{depth_um}.npz (e.g., p1250_d100.npz)\"\n",
    "    )\n",
    "\n",
    "# Load FDTD data files (Requirement 1.1)\n",
    "npz_files = sorted(data_dir.glob(\"p*_d*.npz\"))\n",
    "if len(npz_files) < 2:\n",
    "    raise ValueError(\n",
    "        f\"Insufficient data files: {len(npz_files)} found, need at least 2.\\n\"\n",
    "        f\"Please add FDTD .npz files to {data_dir}\"\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(npz_files)} FDTD data files:\")\n",
    "for f in npz_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# Create FDTD data loader (Phase 2 Service)\n",
    "loader = FDTDDataLoaderService(data_dir=data_dir)\n",
    "\n",
    "# Load elastic constants (for dimensionless scaling)\n",
    "elastic_lambda = 51.2e9  # Pa (Aluminum 6061-T6)\n",
    "elastic_mu = 26.1e9      # Pa\n",
    "density = 2700.0         # kg/m³\n",
    "\n",
    "# Load first file to estimate displacement scale\n",
    "sample_data = loader.load_file(npz_files[0])\n",
    "U_ref = np.std(np.concatenate([sample_data.Ux, sample_data.Uy]))\n",
    "\n",
    "print(f\"\\nEstimated U_ref: {U_ref:.2e} m\")\n",
    "\n",
    "# Task 2.2: Create characteristic scales (Requirement 1.3)\n",
    "scales = CharacteristicScales.from_physics(\n",
    "    domain_length=0.04,  # 40mm\n",
    "    elastic_lambda=elastic_lambda,\n",
    "    elastic_mu=elastic_mu,\n",
    "    density=density,\n",
    "    displacement_amplitude=U_ref\n",
    ")\n",
    "\n",
    "# Create scaler\n",
    "scaler = DimensionlessScalerService(scales)\n",
    "\n",
    "print(f\"\\nCharacteristic scales:\")\n",
    "print(f\"  L_ref = {scales.L_ref:.4f} m\")\n",
    "print(f\"  T_ref = {scales.T_ref:.2e} s\")\n",
    "print(f\"  U_ref = {scales.U_ref:.2e} m\")\n",
    "print(f\"  σ_ref = {scales.sigma_ref:.2e} Pa\")\n",
    "print(f\"  c_l   = {scales.velocity_ref:.0f} m/s\")\n",
    "\n",
    "# Load and normalize data (Requirement 1.2)\n",
    "start_time = time.time()\n",
    "dataset = loader.load_multiple_files(npz_files, apply_dimensionless=True, scaler=scaler)\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(dataset.x)} samples from {len(npz_files)} file(s) in {load_time:.2f}s\")\n",
    "\n",
    "# Task 2.2: Display dimensionless variable ranges (Requirement 1.4)\n",
    "print(f\"\\nData ranges (dimensionless):\")\n",
    "print(f\"  x̃:  [{np.min(dataset.x):.3f}, {np.max(dataset.x):.3f}]\")\n",
    "print(f\"  ỹ:  [{np.min(dataset.y):.3f}, {np.max(dataset.y):.3f}]\")\n",
    "print(f\"  t̃:  [{np.min(dataset.t):.3f}, {np.max(dataset.t):.3f}]\")\n",
    "print(f\"  T̃1: [{np.min(dataset.T1):.3f}, {np.max(dataset.T1):.3f}]\")\n",
    "print(f\"  T̃3: [{np.min(dataset.T3):.3f}, {np.max(dataset.T3):.3f}]\")\n",
    "print(f\"  Ũx: [{np.min(dataset.Ux):.3f}, {np.max(dataset.Ux):.3f}]\")\n",
    "print(f\"  Ũy: [{np.min(dataset.Uy):.3f}, {np.max(dataset.Uy):.3f}]\")\n",
    "\n",
    "# Task 2.3: Train/val split (Requirement 1.5)\n",
    "train_data, val_data = loader.train_val_split(dataset, train_ratio=0.8, seed=SEED)\n",
    "print(f\"\\nTrain samples: {len(train_data.x)}\")\n",
    "print(f\"Val samples: {len(val_data.x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3: Visualize spatiotemporal distribution (Requirement 1.6)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(dataset.t, dataset.x, s=0.5, alpha=0.3, c='blue')\n",
    "ax.set_xlabel(\"t̃ (dimensionless time)\", fontsize=12)\n",
    "ax.set_ylabel(\"x̃ (dimensionless x-coordinate)\", fontsize=12)\n",
    "ax.set_title(\"Spatiotemporal Sampling Distribution\", fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Data preparation complete (Tasks 2.1, 2.2, 2.3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7yh6jezaqhe",
   "source": "## Step 2: 2D PINNモデルの構築\n\nこのステップでは、Phase 2のPINNModelBuilder2DServiceを使用して2D PINNモデルを構築します。\n\n### タスク3.1: 設定パラメータ定義とモデル構築\n- 設定パラメータの定義（Requirement 2.1）\n- 5D入力、4D出力のPINNモデル構築（Requirement 2.2）\n- PDEDefinition2DServiceのPDE関数適用（Requirement 2.3）\n- API整合性確保（Requirement 8.3）\n\n**推奨実行時間**: 約10-30秒（モデル構築のみ、訓練は次ステップ）",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ykpkrguj139",
   "source": "# Task 3.1: Configuration parameters definition (Requirement 2.1)\n# Define PINN configuration parameters using Python dictionary format\n\nconfig = {\n    \"network\": {\n        # 5D input: (x̃, ỹ, t̃, pitch_norm, depth_norm) (Requirement 2.2)\n        # 4D output: (T̃1, T̃3, Ũx, Ũy)\n        \"layer_sizes\": [5, 64, 64, 64, 4],\n        \"activation\": \"tanh\"  # tanh activation for smooth approximation\n    },\n    \"training\": {\n        \"epochs\": 5000,  # Minimum 1000, recommended 5000 (Requirement 2.4)\n        \"learning_rate\": 0.001,  # Initial learning rate\n        \"loss_weights\": {\n            \"data\": 1.0,   # Weight for data fitting loss (L_data)\n            \"pde\": 1.0,    # Weight for PDE residual loss (L_pde)\n            \"bc\": 0.0      # Weight for boundary condition loss (L_bc)\n        }\n    }\n}\n\nprint(\"Configuration parameters:\")\nprint(f\"  Network architecture: {config['network']['layer_sizes']}\")\nprint(f\"  Input dimension: {config['network']['layer_sizes'][0]} (x̃, ỹ, t̃, pitch_norm, depth_norm)\")\nprint(f\"  Output dimension: {config['network']['layer_sizes'][-1]} (T̃1, T̃3, Ũx, Ũy)\")\nprint(f\"  Hidden layers: {config['network']['layer_sizes'][1:-1]}\")\nprint(f\"  Activation: {config['network']['activation']}\")\nprint(f\"  Epochs: {config['training']['epochs']}\")\nprint(f\"  Learning rate: {config['training']['learning_rate']}\")\nprint(f\"  Loss weights: L_data={config['training']['loss_weights']['data']}, L_pde={config['training']['loss_weights']['pde']}, L_bc={config['training']['loss_weights']['bc']}\")\n\n# Verify layer dimensions (Requirement 2.2)\nassert config[\"network\"][\"layer_sizes\"][0] == 5, \"Input dimension must be 5\"\nassert config[\"network\"][\"layer_sizes\"][-1] == 4, \"Output dimension must be 4\"\nprint(\"\\n✓ Configuration validated\")\n\n# Task 3.1: Model construction (Requirements 2.2, 8.3)\nprint(\"\\nConstructing PINN model...\")\n\n# Create model builder service (Phase 2 implementation)\nbuilder = PINNModelBuilder2DService()\n\n# Create PDE definition service for physics constraints (Requirement 2.3)\npde_service = PDEDefinition2DService(\n    elastic_lambda=elastic_lambda,\n    elastic_mu=elastic_mu,\n    density=density,\n    scales=scales\n)\n\n# Build 2D PINN model with 5D input, 4D output\n# This creates a DeepXDE model with the specified architecture\nmodel = builder.build_model(config)\n\nprint(\"✓ PINN model constructed successfully\")\nprint(f\"  Model type: {type(model)}\")\n\n# Get PDE function for physics constraints (Requirement 2.3)\npde_function = pde_service.create_pde_function()\n\nprint(\"✓ PDE function created for physics constraints\")\nprint(f\"  PDE type: Dimensionless 2D elastic wave equation\")\n\n# Verify model construction\nassert model is not None, \"Model construction failed\"\nprint(\"\\n✓ Model construction complete (Task 3.1)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eomwto9fe6p",
   "source": "## Step 3: 訓練実行と損失監視\n\nこのステップでは、TrainingPipelineServiceを使用してPINN訓練を実行し、損失項をリアルタイムで監視します。\n\n### タスク4.1: 訓練実行と損失ロギング\n- LossLoggingCallback、R2ValidationCallback、DivergenceDetectionCallbackの設定\n- TrainingPipelineServiceによる訓練実行（Requirements 2.4, 2.5）\n- 訓練時間の記録（GPU使用時）（Requirement 2.9）\n- 個別損失項（L_data, L_pde, L_bc）とtotal lossの表示（Requirement 2.6）\n\n### タスク4.2: 訓練履歴プロットとNaN検出\n- PlotGeneratorServiceによる訓練履歴の4系列プロット（Requirement 2.7）\n- DivergenceDetectionCallbackによるNaN loss検出（Requirement 2.8）\n- 訓練収束性の視覚的確認\n\n**推奨実行時間**: GPU: 約10-15分（5000 epochs）、CPU: 約60-90分",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "xcpd1elvln",
   "source": "# Task 4.1: Training execution and loss logging (Requirements 2.4, 2.5, 2.6, 2.9)\n\n# Prepare validation data for R2ValidationCallback\n# Validation input: (N_val, 5) [x, y, t, pitch_norm, depth_norm]\nval_x = np.column_stack([\n    val_data.x,\n    val_data.y,\n    val_data.t,\n    val_data.pitch_norm,\n    val_data.depth_norm\n])\n\n# Validation output: (N_val, 4) [T1, T3, Ux, Uy]\nval_y = np.column_stack([\n    val_data.T1,\n    val_data.T3,\n    val_data.Ux,\n    val_data.Uy\n])\n\nprint(f\"Validation data prepared:\")\nprint(f\"  val_x shape: {val_x.shape}\")\nprint(f\"  val_y shape: {val_y.shape}\")\n\n# Create output directory for training artifacts\noutput_dir = Path.cwd().parent / \"experiments\" / \"forward_validation\"\noutput_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"\\nOutput directory: {output_dir}\")\n\n# Configure callbacks (Requirement 2.5, 2.8)\ncallbacks = [\n    LossLoggingCallback(log_interval=100),  # Log every 100 epochs (Requirement 2.6)\n    R2ValidationCallback(\n        val_x=val_x,\n        val_y=val_y,\n        r2_threshold=0.9,\n        log_interval=1000  # Compute R² every 1000 epochs\n    ),\n    DivergenceDetectionCallback(\n        output_dir=output_dir,\n        nan_threshold=1e10  # Halt if loss exceeds threshold (Requirement 2.8)\n    )\n]\n\nprint(\"\\n✓ Callbacks configured:\")\nprint(\"  - LossLoggingCallback (log_interval=100)\")\nprint(\"  - R2ValidationCallback (r2_threshold=0.9, log_interval=1000)\")\nprint(\"  - DivergenceDetectionCallback (nan_threshold=1e10)\")\n\n# Create training pipeline service\ntraining_pipeline = TrainingPipelineService()\n\n# Record training start time (Requirement 2.9)\nprint(f\"\\nStarting training with {config['training']['epochs']} epochs...\")\nprint(f\"GPU available: {torch.cuda.is_available()}\")\ntraining_start_time = time.time()\n\n# Execute training (Requirement 2.4)\ntrained_model, history = training_pipeline.train(\n    model=model,\n    config=config,\n    output_dir=output_dir,\n    callbacks=callbacks\n)\n\n# Record training end time (Requirement 2.9)\ntraining_end_time = time.time()\ntraining_duration = training_end_time - training_start_time\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Training Complete\")\nprint(f\"{'='*60}\")\nprint(f\"Training time: {training_duration:.2f} seconds ({training_duration/60:.2f} minutes)\")\nprint(f\"Epochs completed: {config['training']['epochs']}\")\nprint(f\"Final losses:\")\nif history:\n    if \"L_data\" in history and len(history[\"L_data\"]) > 0:\n        print(f\"  L_data: {history['L_data'][-1]:.6e}\")\n    if \"L_pde\" in history and len(history[\"L_pde\"]) > 0:\n        print(f\"  L_pde: {history['L_pde'][-1]:.6e}\")\n    if \"L_bc\" in history and len(history[\"L_bc\"]) > 0:\n        print(f\"  L_bc: {history['L_bc'][-1]:.6e}\")\n    if \"total_loss\" in history and len(history[\"total_loss\"]) > 0:\n        print(f\"  Total: {history['total_loss'][-1]:.6e}\")\n\nprint(\"\\n✓ Training execution complete (Task 4.1)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8l7jb2550gi",
   "source": "# Task 4.2: Training history plot and convergence check (Requirement 2.7)\n\n# Create plot generator service\nplot_generator = PlotGeneratorService()\n\n# Plot training curves with 4 series: L_data, L_pde, L_bc, Total loss (Requirement 2.7)\nprint(\"Generating training history plot...\")\n\nfig, ax = plot_generator.plot_training_curves(\n    history=history,\n    save_path=output_dir / \"training_history.png\",\n    log_scale=False  # Linear scale for better visual interpretation\n)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"✓ Training history plot saved to: {output_dir / 'training_history.png'}\")\n\n# Visual convergence check (Requirement 2.7)\nprint(\"\\nTraining convergence analysis:\")\nif history and \"total_loss\" in history and len(history[\"total_loss\"]) > 100:\n    initial_loss = history[\"total_loss\"][0]\n    final_loss = history[\"total_loss\"][-1]\n    loss_reduction = (initial_loss - final_loss) / initial_loss * 100\n    \n    print(f\"  Initial total loss: {initial_loss:.6e}\")\n    print(f\"  Final total loss: {final_loss:.6e}\")\n    print(f\"  Loss reduction: {loss_reduction:.2f}%\")\n    \n    if loss_reduction > 50:\n        print(\"  ✓ Good convergence: Loss reduced by >50%\")\n    elif loss_reduction > 20:\n        print(\"  ⚠ Moderate convergence: Loss reduced by >20% but <50%\")\n        print(\"    Recommendation: Consider increasing epochs or adjusting learning rate\")\n    else:\n        print(\"  ✗ Poor convergence: Loss reduced by <20%\")\n        print(\"    Recommendation: Check loss weights, learning rate, or network architecture\")\nelse:\n    print(\"  Warning: Insufficient training history for convergence analysis\")\n\n# Check if DivergenceDetectionCallback detected any issues (Requirement 2.8)\ndivergence_diagnostic_path = output_dir / \"divergence_diagnostic.json\"\nif divergence_diagnostic_path.exists():\n    print(\"\\n⚠ WARNING: NaN loss detected during training!\")\n    print(f\"  Diagnostic file: {divergence_diagnostic_path}\")\n    print(\"  Recommendations:\")\n    print(\"    - Reduce learning rate (try 0.0001 instead of 0.001)\")\n    print(\"    - Adjust loss weights (try w_pde=0.1 if PDE loss dominates)\")\n    print(\"    - Check input data normalization\")\nelse:\n    print(\"\\n✓ No NaN loss detected - training was stable\")\n\nprint(\"\\n✓ Training history visualization complete (Task 4.2)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}